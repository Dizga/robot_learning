diff --git a/.DS_Store b/.DS_Store
deleted file mode 100644
index d256e9d..0000000
Binary files a/.DS_Store and /dev/null differ
diff --git a/.gitignore b/.gitignore
index 4310416..1f7be83 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,4 +1,4 @@
-hw1/data
+**/data
 /outputs
 /venv
 .DS_Store
diff --git a/.vscode/launch.json b/.vscode/launch.json
new file mode 100644
index 0000000..3c27eb6
--- /dev/null
+++ b/.vscode/launch.json
@@ -0,0 +1,25 @@
+{
+    // Use IntelliSense to learn about possible attributes.
+    // Hover to view descriptions of existing attributes.
+    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
+    "version": "0.2.0",
+    "configurations": [
+        
+        {
+            "name": "HW2",
+            "type": "debugpy",
+            "request": "launch",
+            // "program": "${file}",
+            "program": "run_hw2_mb.py",
+            "console": "integratedTerminal"
+        },
+        {
+            "name": "HW2 args",
+            "type": "debugpy",
+            "request": "launch",
+            "program": "run_hw2_mb.py",
+            "console": "integratedTerminal",
+            "args": "${command:pickArgs}"
+        }
+    ]
+}
\ No newline at end of file
diff --git a/cmd.txt b/cmd.txt
new file mode 100644
index 0000000..bec71c8
--- /dev/null
+++ b/cmd.txt
@@ -0,0 +1,84 @@
+env.exp_name=q1_cheetah_n500_arch1x32 env.env_name=cheetah-roble-v0 alg.num_agent_train_steps_per_iter=500 alg.network.layer_sizes='[32]' alg.n_iter=1
+env.exp_name=q1_cheetah_n500_arch1x32_tanh env.env_name=cheetah-roble-v0 alg.num_agent_train_steps_per_iter=500 alg.network.layer_sizes='[32]' alg.network.output_activation=tanh alg.n_iter=1
+env.exp_name=q1_cheetah_n500_arch1x32 env.env_name=cheetah-roble-v0 alg.num_agent_train_steps_per_iter=500 alg.network.layer_sizes='[32]' alg.n_iter=1
+
+env.exp_name=q1_cheetah_n500_arch1x32 
+env.env_name=cheetah-roble-v0 
+alg.num_agent_train_steps_per_iter=500 
+alg.network.layer_sizes='[32]'
+alg.n_iter=1
+logging.video_log_freq=1
+
+env.exp_name=q1_cheetah_n5_arch2x256 
+env.env_name=cheetah-roble-v0 
+alg.num_agent_train_steps_per_iter=5
+alg.network.layer_sizes='[256,256]'
+alg.n_iter=1
+logging.video_log_freq=1
+
+env.exp_name=q1_cheetah_n500_arch2x256 
+env.env_name=cheetah-roble-v0 
+alg.num_agent_train_steps_per_iter=500
+alg.network.layer_sizes='[256,256]'
+alg.n_iter=1
+logging.video_log_freq=1
+
+
+env.exp_name=q2_obstacles_singleiteration
+env.env_name=obstacles-roble-v0
+alg.num_agent_train_steps_per_iter=20
+alg.batch_size_initial=5000
+alg.batch_size=1000
+alg.mpc_horizon=10
+alg.n_iter=1
+logging.video_log_freq=1
+
+env.exp_name=q3_obstacles
+env.env_name=obstacles-roble-v0
+alg.num_agent_train_steps_per_iter=20
+alg.batch_size_initial=5000
+alg.batch_size=1000
+alg.mpc_horizon=10
+alg.n_iter=12
+logging.video_log_freq=11
+
+env.exp_name=q3_reacher
+env.env_name=reacher-roble-v0
+alg.mpc_horizon=10
+alg.num_agent_train_steps_per_iter=1000
+alg.batch_size_initial=5000
+alg.batch_size=5000
+alg.n_iter=15
+logging.video_log_freq=14
+
+env.exp_name=q3_cheetah env.env_name=cheetah-roble-v0 alg.mpc_horizon=15  alg.num_agent_train_steps_per_iter=1500 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=20 logging.video_log_freq=-1
+
+
+env.exp_name=q5_cheetah_cem_2
+env.env_name='cheetah-roble-v0'
+alg.mpc_horizon=15
+alg.add_sl_noise=true
+alg.num_agent_train_steps_per_iter=1500
+alg.batch_size_initial=5000
+alg.batch_size=5000
+alg.n_iter=5
+logging.video_log_freq=4
+alg.mpc_action_sampling_strategy='cem'
+alg.cem_iterations=2
+
+python run_hw2_mb.py env.exp_name=q5_cheetah_cem_2 env.env_name='cheetah-roble-v0' alg.mpc_horizon=15 alg.add_sl_noise=true alg.num_agent_train_steps_per_iter=1500 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=5 logging.video_log_freq=4 alg.mpc_action_sampling_strategy='cem' cem_iterations=2
+
+
+env.exp_name=q5_cheetah_cem_4
+env.env_name='cheetah-roble-v0'
+alg.mpc_horizon=15
+alg.add_sl_noise=true
+alg.num_agent_train_steps_per_iter=1500
+alg.batch_size_initial=5000
+alg.batch_size=5000
+alg.n_iter=5
+logging.video_log_freq=4
+alg.mpc_action_sampling_strategy='cem'
+alg.cem_iterations=4
+
+python run_hw2_mb.py env.exp_name=q5_cheetah_cem_4 env.env_name='cheetah-roble-v0' alg.mpc_horizon=15 alg.add_sl_noise=true alg.num_agent_train_steps_per_iter=1500 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=5 logging.video_log_freq=4 alg.mpc_action_sampling_strategy='cem' cem_iterations=4
\ No newline at end of file
diff --git a/conf/config_hw2.yaml b/conf/config_hw2.yaml
index 9393815..be4392a 100644
--- a/conf/config_hw2.yaml
+++ b/conf/config_hw2.yaml
@@ -1,7 +1,7 @@
 env:
     env_name: 'cheetah-roble-v0' # ['cheetah-roble-v0', 'reacher-roble-v0', 'obstacles-roble-v0' ]
     max_episode_length: 200
-    exp_name: 'todo'
+    exp_name: 'q1_cheetah_n500_arch1x32'
 
 alg:
     n_iter: 5
@@ -13,7 +13,7 @@ alg:
     cem_num_elites: 5
     cem_alpha: 1
     add_sl_noise: True
-    num_agent_train_steps_per_iter: 1000
+    num_agent_train_steps_per_iter: 500
     batch_size_initial: 20000
     batch_size: 8000
     train_batch_size: 512
diff --git a/hw2/README.md b/hw2/README.md
new file mode 100644
index 0000000..b7b50a0
--- /dev/null
+++ b/hw2/README.md
@@ -0,0 +1,99 @@
+# Commands used to make the experiments.
+
+## Question 1
+
+
+Network of 1x32 hidden layers, with 500 training steps
+```
+python run_hw2_mb.py env.exp_name=q1_cheetah_n500_arch1x32 env.env_name=cheetah-roble-v0 alg.num_agent_train_steps_per_iter=500 alg.network.layer_sizes='[32]' alg.n_iter=1
+```
+
+Network of 2x256 hidden layers, with 5 training steps
+```
+python run_hw2_mb.py env.exp_name=q1_cheetah_n5_arch2x256 env.env_name=cheetah-roble-v0 alg.num_agent_train_steps_per_iter=5 alg.network.layer_sizes='[256,256]' alg.n_iter=1 logging.video_log_freq=1
+```
+
+Network of 2x256 hidden layers, with 500 training steps
+```
+python run_hw2_mb.py env.exp_name=q1_cheetah_n500_arch2x256 env.env_name=cheetah-roble-v0 alg.num_agent_train_steps_per_iter=500 alg.network.layer_sizes='[256,256]' alg.n_iter=1 logging.video_log_freq=1
+```
+
+## Question 2
+
+```
+python run_hw2_mb.py env.exp_name=q2_obstacles_singleiteration env.env_name=obstacles-roble-v0 alg.num_agent_train_steps_per_iter=20 alg.batch_size_initial=5000 alg.batch_size=1000 alg.mpc_horizon=10 alg.n_iter=1 logging.video_log_freq=1
+```
+
+## Question 3
+
+
+MBRL run on obstacles env.
+```
+python run_hw2_mb.py env.exp_name=q3_obstacles env.env_name=obstacles-roble-v0 alg.num_agent_train_steps_per_iter=20 alg.batch_size_initial=5000 alg.batch_size=1000 alg.mpc_horizon=10 alg.n_iter=12 logging.video_log_freq=11
+```
+
+MBRL run on reacher env.
+```
+python run_hw2_mb.py env.exp_name=q3_reacher env.env_name=reacher-roble-v0 alg.mpc_horizon=10 alg.num_agent_train_steps_per_iter=1000 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=15 logging.video_log_freq=14
+```
+
+MBRL run on cheetah env.
+```
+python run_hw2_mb.py env.exp_name=q3_cheetah env.env_name=cheetah-roble-v0 alg.mpc_horizon=15  alg.num_agent_train_steps_per_iter=1500 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=20 logging.video_log_freq=19
+```
+
+## Question 4
+
+
+Horizon 5 steps.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_horizon5 env.env_name=reacher-roble-v0 alg.add_sl_noise=true alg.mpc_horizon=5 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 logging.video_log_freq=14
+```
+
+Horizon 15 steps.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_horizon15 env.env_name=reacher-roble-v0 alg.add_sl_noise=true alg.mpc_horizon=15 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 logging.video_log_freq=14
+```
+
+Horizon 30 steps.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_horizon30 env.env_name=reacher-roble-v0 alg.add_sl_noise=true alg.mpc_horizon=30 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 logging.video_log_freq=14
+```
+
+100 action sequences.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_numseq100 env.env_name=reacher-roble-v0 alg.add_sl_noise=true alg.mpc_horizon=10 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 alg.mpc_num_action_sequences=100 logging.video_log_freq=14
+```
+
+1000 action sequences.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_numseq1000 env.env_name=reacher-roble-v0 alg.add_sl_noise=true alg.mpc_horizon=10 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 alg.mpc_num_action_sequences=1000 logging.video_log_freq=14
+```
+
+No ensemble.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_ensemble1 env.env_name=reacher-roble-v0 alg.ensemble_size=1 alg.add_sl_noise=true alg.mpc_horizon=10 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 logging.video_log_freq=14
+```
+
+Ensemble size 3.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_ensemble3 env.env_name=reacher-roble-v0 alg.ensemble_size=3 alg.add_sl_noise=true alg.mpc_horizon=10 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 logging.video_log_freq=14
+```
+
+## Question 5
+
+MBRL with random shooting
+```
+python run_hw2_mb.py env.exp_name=q5_cheetah_random env.env_name='cheetah-roble-v0' alg.mpc_horizon=15  alg.num_agent_train_steps_per_iter=1500 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=5 logging.video_log_freq=4 alg.mpc_action_sampling_strategy='random'
+```
+
+MBRL with CEM 2
+```
+python run_hw2_mb.py env.exp_name=q5_cheetah_cem_2 env.env_name='cheetah-roble-v0' alg.mpc_horizon=15 alg.add_sl_noise=true alg.num_agent_train_steps_per_iter=1500 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=5 logging.video_log_freq=4 alg.mpc_action_sampling_strategy='cem' alg.cem_iterations=2
+```
+
+
+MBRL with CEM 4
+```
+python run_hw2_mb.py env.exp_name=q5_cheetah_cem_4 env.env_name='cheetah-roble-v0' alg.mpc_horizon=15 alg.add_sl_noise=true alg.num_agent_train_steps_per_iter=1500 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=5 logging.video_log_freq=4 alg.mpc_action_sampling_strategy='cem' alg.cem_iterations=4
+```
\ No newline at end of file
diff --git a/hw2/plots_generator.py b/hw2/plots_generator.py
new file mode 100644
index 0000000..89ce796
--- /dev/null
+++ b/hw2/plots_generator.py
@@ -0,0 +1,119 @@
+import os
+import pandas as pd
+import matplotlib.pyplot as plt
+import numpy as np
+
+
+def generate_plots(base_dir='hw2/data'):
+    for root, dirs, files in os.walk(base_dir):
+        for file in files:
+            if file == 'log_data.csv':
+                csv_path = os.path.join(root, file)
+                log_data = pd.read_csv(csv_path)
+                
+                eval_returns = log_data['trainer/Eval_AverageReturn']
+                train_returns = log_data['trainer/Train_AverageReturn']
+                
+                x_values = np.arange(1,1+len(eval_returns))
+
+                plt.plot(x_values, eval_returns, 'o-', label='Evaluation Returns')
+                plt.plot(x_values, train_returns, 'o-', label='Training Returns')
+
+                plt.legend()
+                plt.title('Training and Evaluation Returns Over Time')
+                plt.xlabel('Epochs')
+                plt.ylabel('Average Return')
+                plt.xticks(x_values)
+
+                plt.show()
+
+def generate_q4_plot(base_dir='hw2/data'):
+
+    horizon5_path = os.path.join(base_dir, 'hw2_q4_reacher_horizon5/log_data.csv')
+    horizon15_path = os.path.join(base_dir, 'hw2_q4_reacher_horizon15/log_data.csv')
+    horizon30_path = os.path.join(base_dir, 'hw2_q4_reacher_horizon30/log_data.csv')
+
+    numseq100_path = os.path.join(base_dir, 'hw2_q4_reacher_numseq100/log_data.csv')
+    numseq1000_path = os.path.join(base_dir, 'hw2_q4_reacher_numseq1000/log_data.csv')
+
+    ensemble1_path = os.path.join(base_dir, 'hw2_q4_reacher_ensemble1/log_data.csv')
+    ensemble3_path = os.path.join(base_dir, 'hw2_q4_reacher_ensemble3/log_data.csv')
+
+    horizon5_returns = pd.read_csv(horizon5_path)['trainer/Eval_AverageReturn']
+    horizon15_returns = pd.read_csv(horizon15_path)['trainer/Eval_AverageReturn']
+    horizon30_returns = pd.read_csv(horizon30_path)['trainer/Eval_AverageReturn']
+
+    x_values = np.arange(1,1+len(horizon5_returns))
+
+    plt.plot(x_values, horizon5_returns, label='5 steps')
+    plt.plot(x_values, horizon15_returns, label='15 steps')
+    plt.plot(x_values, horizon30_returns, label='30 steps')
+
+    plt.legend()
+    plt.title('Return with differents horizon steps')
+    plt.xlabel('Epochs')
+    plt.ylabel('Average Return')
+    plt.xticks(x_values)
+
+    plt.show()
+    plt.clf()
+
+    numseq100_returns = pd.read_csv(numseq100_path)['trainer/Eval_AverageReturn']
+    numseq1000_returns = pd.read_csv(numseq1000_path)['trainer/Eval_AverageReturn']
+
+    plt.plot(x_values, numseq100_returns, label='100 sequences')
+    plt.plot(x_values, numseq1000_returns, label='1000 sequences')
+
+    plt.legend()
+    plt.title('Return with differents actions sequences')
+    plt.xlabel('Epochs')
+    plt.ylabel('Average Return')
+    plt.xticks(x_values)
+
+    plt.show()
+    plt.clf()
+
+    ensemble1_returns = pd.read_csv(ensemble1_path)['trainer/Eval_AverageReturn']
+    ensemble3_returns = pd.read_csv(ensemble3_path)['trainer/Eval_AverageReturn']
+
+    plt.plot(x_values, ensemble1_returns, label='No ensemble')
+    plt.plot(x_values, ensemble3_returns, label='Ensemble of 3')
+
+    plt.legend()
+    plt.title('Return with differents ensembles')
+    plt.xlabel('Epochs')
+    plt.ylabel('Average Return')
+    plt.xticks(x_values)
+
+    plt.show()
+
+
+
+def generate_q5_plot(base_dir='hw2/data'):
+
+    rs_file = os.path.join(base_dir, 'hw2_q5_cheetah_random/log_data.csv')
+    cem2_file = os.path.join(base_dir, 'hw2_q5_cheetah_cem_2/log_data.csv')
+    cem4_file = os.path.join(base_dir, 'hw2_q5_cheetah_cem_4/log_data.csv')
+
+    rs_returns = pd.read_csv(rs_file)['trainer/Eval_AverageReturn']
+    cem2_returns = pd.read_csv(cem2_file)['trainer/Eval_AverageReturn']
+    cem4_returns = pd.read_csv(cem4_file)['trainer/Eval_AverageReturn']
+
+    x_values = np.arange(1,1+len(rs_returns))
+
+    plt.plot(x_values, rs_returns, label='Random shooting')
+    plt.plot(x_values, cem2_returns, label='CEM 2')
+    plt.plot(x_values, cem4_returns, label='CEM 4')
+
+    plt.legend()
+    plt.title('Evaluation Returns Over Time')
+    plt.xlabel('Epochs')
+    plt.ylabel('Average Return')
+    plt.xticks(x_values)
+
+    plt.show()
+
+if __name__ == '__main__':
+    generate_plots()
+    generate_q4_plot()
+    generate_q5_plot()
diff --git a/hw2/roble/agents/mb_agent.py b/hw2/roble/agents/mb_agent.py
index 28144f8..7bd731e 100644
--- a/hw2/roble/agents/mb_agent.py
+++ b/hw2/roble/agents/mb_agent.py
@@ -14,7 +14,7 @@ import numpy as np
 class MBAgent(BaseAgent):
     import hw1.roble.util.class_util as classu
     @classu.hidden_member_initialize
-    def __init__(self, env, params):
+    def __init__(self, env, params, **kwargs):
         super(MBAgent, self).__init__()
 
         self._env = env.unwrapped
@@ -41,16 +41,21 @@ class MBAgent(BaseAgent):
         num_data = ob_no.shape[0]
         num_data_per_ens = int(num_data / self._params["ensemble_size"])
 
+        # perm = np.random.permutation(len(ob_no))
+
         for i in range(self._params["ensemble_size"]):
             # select which datapoints to use for this model of the ensemble
             # you might find the num_data_per_env variable defined above useful
 
-            # observations = # TODO(Q1)
-            # actions = # TODO(Q1)
-            # next_observations = # TODO(Q1)
+            start_idx = i * num_data_per_ens
+            end_idx = start_idx + num_data_per_ens
+
+            observations = ob_no[start_idx:end_idx]
+            actions = ac_na[start_idx:end_idx]
+            next_observations = next_ob_no[start_idx:end_idx]
 
             # # use datapoints to update one of the dyn_models
-            # model =  # TODO(Q1)
+            model =  self._dyn_models[i]
             log = model.update(observations, actions, next_observations,
                                 self._data_statistics)
             loss = log['Training Loss']
diff --git a/hw2/roble/envs/obstacles/obstacles_env.py b/hw2/roble/envs/obstacles/obstacles_env.py
index 19f7f83..5330670 100644
--- a/hw2/roble/envs/obstacles/obstacles_env.py
+++ b/hw2/roble/envs/obstacles/obstacles_env.py
@@ -5,6 +5,8 @@ from gym import spaces
 class Obstacles(gym.Env):
     def __init__(self, start=[-0.5, 0.75], end=[0.7, -0.8], random_starts=True):
 
+        import matplotlib
+        matplotlib.use('Agg')
         import matplotlib.pyplot as plt #inside, so doesnt get imported when not using this env
         self.plt = plt
 
@@ -72,7 +74,9 @@ class Obstacles(gym.Env):
 
         #clear
         self.counter = 0
-        self.plt.clf()
+        # self.plt.clf()
+        self.plt.close(self.fig)
+        self.fig = self.plt.figure()
 
         #return
         return self._get_obs()
@@ -189,6 +193,7 @@ class Obstacles(gym.Env):
         self.fig.canvas.draw()
         img = np.fromstring(self.fig.canvas.tostring_rgb(), dtype=np.uint8)
         img = img.reshape(self.fig.canvas.get_width_height()[::-1] + (3,))
+        # img = img.reshape(960,1280,3)
         return img
 
     def is_valid(self, dat):
diff --git a/hw2/roble/infrastructure/rl_trainer.py b/hw2/roble/infrastructure/rl_trainer.py
index 9187004..8c05ef3 100644
--- a/hw2/roble/infrastructure/rl_trainer.py
+++ b/hw2/roble/infrastructure/rl_trainer.py
@@ -129,6 +129,7 @@ class RL_Trainer(RL_Trainer):
                 if self._params['logging']['save_params']:
                     self._agent.save('{}/agent_itr_{}.pt'.format(self._params['logging']['logdir'], itr))
         
+        self.plot_return()
         return self._logger.get_table_dict()
 
     def collect_training_trajectories(
@@ -279,7 +280,7 @@ class RL_Trainer(RL_Trainer):
         # plot the predictions
         self._fig.clf()
         for i in range(ob_dim):
-            plt.subplot(ob_dim/2, 2, i+1)
+            plt.subplot(int(ob_dim/2), 2, i+1)
             plt.plot(true_states[:,i], 'g')
             plt.plot(pred_states[:,i], 'r')
         self._fig.suptitle('MPE: ' + str(mpe))
@@ -292,3 +293,23 @@ class RL_Trainer(RL_Trainer):
         plt.plot(all_losses)
         self._fig.savefig(self._params['logging']['logdir']+'/itr_'+str(itr)+'_losses.png', dpi=200, bbox_inches='tight')
 
+
+    def plot_return(self):
+        import matplotlib.pyplot as plt
+        self._fig = plt.figure()
+
+        eval_returns = self._logger.get_table_dict()['trainer/Eval_AverageReturn']
+        train_returns = self._logger.get_table_dict()['trainer/Train_AverageReturn']
+        x_values = np.arange(1,1+len(eval_returns))
+
+        plt.plot(x_values, eval_returns, 'go-', label='Evaluation Returns')
+        plt.plot(x_values, train_returns, 'bo-', label='Training Returns')
+
+        plt.legend()
+
+        plt.title('Training and Evaluation Returns Over Time')
+        plt.xlabel('Epoch')
+        plt.ylabel('Average Return')
+        plt.xticks(x_values)
+
+        self._fig.savefig(self._params['logging']['logdir']+'/returns.png', dpi=200, bbox_inches='tight')
\ No newline at end of file
diff --git a/hw2/roble/models/ff_model.py b/hw2/roble/models/ff_model.py
index 1c08b28..8965444 100644
--- a/hw2/roble/models/ff_model.py
+++ b/hw2/roble/models/ff_model.py
@@ -15,7 +15,7 @@ class FFModel(nn.Module, BaseModel):
         self._delta_network = ptu.build_mlp(
             input_size=self._ob_dim + self._ac_dim,
             output_size=self._ob_dim,
-            **params["network"]
+            params=self._network
         )
         self._delta_network.to(ptu.device)
         self._optimizer = optim.Adam(
@@ -73,8 +73,15 @@ class FFModel(nn.Module, BaseModel):
                 unnormalized) output of the delta network. This is needed
         """
         # normalize input data to mean 0, std 1
-        # obs_normalized = # TODO(Q1)
-        # acs_normalized = # TODO(Q1)
+
+        obs_unnormalized = ptu.from_numpy(obs_unnormalized)
+        acs_unnormalized = ptu.from_numpy(acs_unnormalized)
+
+        obs_normalized = normalize(obs_unnormalized, self._obs_mean, self._obs_std)
+        acs_normalized = normalize(acs_unnormalized, self._acs_mean, self._acs_std)
+
+        # obs_normalized = (obs_unnormalized - self._obs_mean) / self._obs_std
+        # acs_normalized = (acs_unnormalized - self._acs_mean) / self._acs_std
 
         # predicted change in obs
         concatenated_input = torch.cat([obs_normalized, acs_normalized], dim=1)
@@ -82,9 +89,27 @@ class FFModel(nn.Module, BaseModel):
         # TODO(Q1) compute delta_pred_normalized and next_obs_pred
         # Hint: as described in the PDF, the output of the network is the
         # *normalized change* in state, i.e. normalized(s_t+1 - s_t).
-        # delta_pred_normalized = # TODO(Q1)
-        # next_obs_pred = # TODO(Q1)
+
+        delta_pred_normalized = self._delta_network(concatenated_input)
+        delta_pred = unnormalize(delta_pred_normalized, self._delta_mean, self._delta_std)
+        # delta_pred = delta_pred_normalized * self._delta_std + self._delta_mean
+
+        next_obs_pred = obs_unnormalized + delta_pred
         return next_obs_pred, delta_pred_normalized
+        
+        # obs_normalized = ptu.from_numpy((obs_unnormalized - obs_mean) / obs_std)
+        # acs_normalized = ptu.from_numpy((acs_unnormalized - acs_mean) / acs_std)
+
+        # # predicted change in obs
+        # concatenated_input = torch.cat([obs_normalized, acs_normalized], dim=1)
+
+        # # TODO(Q1) compute delta_pred_normalized and next_obs_pred
+        # # Hint: as described in the PDF, the output of the network is the
+        # # *normalized change* in state, i.e. normalized(s_t+1 - s_t).
+        # delta_pred_normalized = self._delta_network(concatenated_input)
+        # delta_pred = delta_pred_normalized * ptu.from_numpy(delta_std) + ptu.from_numpy(delta_mean)
+        # next_obs_pred = obs_unnormalized + delta_pred
+        # return next_obs_pred, delta_pred_normalized
 
     def get_prediction(self, obs, acs, data_statistics):
         """
@@ -103,7 +128,13 @@ class FFModel(nn.Module, BaseModel):
         # prediction = # TODO(Q1) get numpy array of the predicted next-states (s_t+1)
         # Hint: `self(...)` returns a tuple, but you only need to use one of the
         # outputs.
-        return prediction
+
+        # self.update_statistics(**data_statistics)
+        # next_obs_pred, _ = self(ptu.from_numpy(obs), ptu.from_numpy(acs), **data_statistics)
+
+        next_obs_pred, _ = self(obs, acs, **data_statistics)
+
+        return ptu.to_numpy(next_obs_pred)
 
     def update(self, observations, actions, next_observations, data_statistics):
         """
@@ -128,6 +159,22 @@ class FFModel(nn.Module, BaseModel):
         # loss = # TODO(Q1) compute the loss
         # Hint: `self(...)` returns a tuple, but you only need to use one of the
         # outputs.
+
+        self.update_statistics(**data_statistics)
+
+        true_delta = ptu.from_numpy(next_observations - observations)
+        # target_norm = (true_delta - self._delta_mean) / self._delta_std
+        target_norm = normalize(true_delta, self._delta_mean, self._delta_std)
+
+        # target_norm = (true_delta - true_delta.mean()) / true_delta.std()
+        # target_norm = ptu.from_numpy(target_norm)
+
+        
+    
+        _, delta_pred_norm = self(observations, actions, **data_statistics)
+
+        loss = self._loss(delta_pred_norm, target_norm)
+
         self._optimizer.zero_grad()
         loss.backward()
         self._optimizer.step()
diff --git a/hw2/roble/policies/MPC_policy.py b/hw2/roble/policies/MPC_policy.py
index 619b622..a5ef657 100644
--- a/hw2/roble/policies/MPC_policy.py
+++ b/hw2/roble/policies/MPC_policy.py
@@ -25,6 +25,8 @@ class MPCPolicy(BasePolicy):
         self._low = self._ac_space.low
         self._high = self._ac_space.high
 
+        # self.models = dyn_models
+
         # Sampling strategy
         allowed_sampling = ('random', 'cem')
         assert self._mpc_action_sampling_strategy in allowed_sampling, f"self._mpc_action_sampling_strategy must be one of the following: {allowed_sampling}"
@@ -44,13 +46,15 @@ class MPCPolicy(BasePolicy):
             # TODO (Q1) uniformly sample trajectories and return an array of
             # dimensions (num_sequences, horizon, self._ac_dim) in the range
             # [self._low, self._high]
-            TODO
+            random_action_sequences = np.random.uniform(low=self._low, high=self._high,
+                                                        size=(num_sequences, horizon, self._ac_space.shape[0]))
             return random_action_sequences
         elif self._mpc_action_sampling_strategy == 'cem':
             # TODO(Q5): Implement action selection using CEM.
             # Begin with randomly selected actions, then refine the sampling distribution
             # iteratively as described in Section 3.3, "Iterative Random-Shooting with Refinement" of
             # https://arxiv.org/pdf/1909.11652.pdf
+
             for i in range(self._cem_iterations):
                 # - Sample candidate sequences from a Gaussian with the current
                 #   elite mean and variance
@@ -60,25 +64,44 @@ class MPCPolicy(BasePolicy):
                 #     (Hint: what existing function can we use to compute rewards for
                 #      our candidate sequences in order to rank them?)
                 # - Update the elite mean and variance
-                pass
+                
+                if i == 0:
+                    candidates = np.random.uniform(low=self._low, high=self._high,
+                                                        size=(num_sequences, horizon, self._ac_space.shape[0]))
+                else:
+                    candidates = np.random.normal(elites_mean, elites_std,(num_sequences, horizon, self._ac_space.shape[0]))
+            
+                scores = self.evaluate_candidate_sequences(candidates, obs)
+
+                elites_ids = scores.argsort()[-self._cem_num_elites:]
+
+                elites = candidates[elites_ids]
+
+                elites_mean = elites.mean(axis=0)
+                elites_std = elites.std(axis=0)
+
+                
 
             # TODO(Q5): Set `cem_action` to the appropriate action sequence chosen by CEM.
             # The shape should be (horizon, self._ac_dim)  
-            cem_action = None
+            cem_action = elites_mean
             return cem_action[None]
         else:
             raise Exception(f"Invalid sample_strategy: {self._mpc_action_sampling_strategy}")
-
+        
     def evaluate_candidate_sequences(self, candidate_action_sequences, obs):
         # TODO(Q2): for each model in ensemble, compute the predicted sum of rewards
         # for each candidate action sequence.
         #
         # Then, return the mean predictions across all ensembles.
         # Hint: the return value should be an array of shape (N,)
+        N, _, _ = candidate_action_sequences.shape
+        predicted_rewards = []
+
         for model in self._dyn_models:
-            pass
+            predicted_rewards.append(self.calculate_sum_of_rewards(obs, candidate_action_sequences, model))
 
-        return TODO
+        return np.array(predicted_rewards).mean(axis=0)
 
     def get_action(self, obs):
         if self._data_statistics is None:
@@ -94,8 +117,9 @@ class MPCPolicy(BasePolicy):
         else:
             predicted_rewards = self.evaluate_candidate_sequences(candidate_action_sequences, obs)
             # pick the action sequence and return the 1st element of that sequence
-            best_action_sequence = None  # TODO (Q2)
-            action_to_take = None  # TODO (Q2)
+            best_sequence_idx = np.argmax(predicted_rewards)
+            best_action_sequence = candidate_action_sequences[best_sequence_idx]
+            action_to_take = best_action_sequence[0]
             return action_to_take[None]  # Unsqueeze the first index
 
     def calculate_sum_of_rewards(self, obs, candidate_action_sequences, model):
@@ -123,4 +147,63 @@ class MPCPolicy(BasePolicy):
         # Hint: Remember that the model can process observations and actions
         #       in batch, which can be much faster than looping through each
         #       action sequence.
+
+        # obs_sequences = []
+        # obs_sequences.append(np.repeat(obs, candidate_action_sequences.shape[0]))
+
+        # cas = np.transpose(candidate_action_sequences, (1, 0, 2))
+        # for actions in cas:
+        #     predicted_obs = model(actions)
+        #     self._env.get_reward(predicted_obs, action)
+
+        # model
+        
+        # return sum_of_rewards
+
+
+
+        # N, H, _ = candidate_action_sequences.shape  # N: Number of sequences, H: Horizon
+        # sum_of_rewards = np.zeros(N)  # Initialize the sum of rewards for each sequence
+        
+        # # Iterate over each action sequence
+        # for i in range(N):
+        #     cumulative_reward = 0  # Initialize cumulative reward for the sequence
+        #     current_obs = obs.copy()  # Start with the initial observation
+            
+        #     # Iterate over each time step in the horizon
+        #     for h in range(H):
+        #         # Extract the current action from the sequence
+        #         action = candidate_action_sequences[i, h, :]
+                
+        #         # Predict the next state based on the current state and action
+        #         next_obs_pred = model.get_prediction(current_obs[None, :], action[None, :], self._data_statistics)
+                
+        #         # Calculate the reward for the predicted state and current action
+        #         reward, _ = self._env.get_reward(current_obs[None, :], action[None, :])
+                
+        #         cumulative_reward += reward  # Accumulate the reward
+                
+        #         # Update the current observation to the predicted next observation
+        #         current_obs = next_obs_pred.squeeze()  # Assuming next_obs_pred is of shape (1, D_obs)
+            
+        #     # Store the sum of rewards for this action sequence
+        #     sum_of_rewards[i] = cumulative_reward
+        
+        # return sum_of_rewards
+
+
+        N, H, _ = candidate_action_sequences.shape
+        candidate_action_sequences = candidate_action_sequences.transpose(1, 0, 2)
+        sum_of_rewards = np.zeros(N)
+        current_obs = np.tile(obs, (N, 1))
+
+        for h in range(H):
+            actions = candidate_action_sequences[h]
+            next_obs_pred = model.get_prediction(current_obs, actions, self._data_statistics)
+
+            rewards, _ = self._env.get_reward(current_obs, actions)
+            sum_of_rewards += rewards.squeeze()
+
+            current_obs = next_obs_pred
+
         return sum_of_rewards
diff --git a/.DS_Store b/.DS_Store
deleted file mode 100644
index d256e9d..0000000
Binary files a/.DS_Store and /dev/null differ
diff --git a/.gitignore b/.gitignore
index 4310416..1f7be83 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,4 +1,4 @@
-hw1/data
+**/data
 /outputs
 /venv
 .DS_Store
diff --git a/.vscode/launch.json b/.vscode/launch.json
new file mode 100644
index 0000000..3c27eb6
--- /dev/null
+++ b/.vscode/launch.json
@@ -0,0 +1,25 @@
+{
+    // Use IntelliSense to learn about possible attributes.
+    // Hover to view descriptions of existing attributes.
+    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
+    "version": "0.2.0",
+    "configurations": [
+        
+        {
+            "name": "HW2",
+            "type": "debugpy",
+            "request": "launch",
+            // "program": "${file}",
+            "program": "run_hw2_mb.py",
+            "console": "integratedTerminal"
+        },
+        {
+            "name": "HW2 args",
+            "type": "debugpy",
+            "request": "launch",
+            "program": "run_hw2_mb.py",
+            "console": "integratedTerminal",
+            "args": "${command:pickArgs}"
+        }
+    ]
+}
\ No newline at end of file
diff --git a/conf/config_hw2.yaml b/conf/config_hw2.yaml
index 9393815..be4392a 100644
--- a/conf/config_hw2.yaml
+++ b/conf/config_hw2.yaml
@@ -1,7 +1,7 @@
 env:
     env_name: 'cheetah-roble-v0' # ['cheetah-roble-v0', 'reacher-roble-v0', 'obstacles-roble-v0' ]
     max_episode_length: 200
-    exp_name: 'todo'
+    exp_name: 'q1_cheetah_n500_arch1x32'
 
 alg:
     n_iter: 5
@@ -13,7 +13,7 @@ alg:
     cem_num_elites: 5
     cem_alpha: 1
     add_sl_noise: True
-    num_agent_train_steps_per_iter: 1000
+    num_agent_train_steps_per_iter: 500
     batch_size_initial: 20000
     batch_size: 8000
     train_batch_size: 512
diff --git a/hw2/README.md b/hw2/README.md
new file mode 100644
index 0000000..b7b50a0
--- /dev/null
+++ b/hw2/README.md
@@ -0,0 +1,99 @@
+# Commands used to make the experiments.
+
+## Question 1
+
+
+Network of 1x32 hidden layers, with 500 training steps
+```
+python run_hw2_mb.py env.exp_name=q1_cheetah_n500_arch1x32 env.env_name=cheetah-roble-v0 alg.num_agent_train_steps_per_iter=500 alg.network.layer_sizes='[32]' alg.n_iter=1
+```
+
+Network of 2x256 hidden layers, with 5 training steps
+```
+python run_hw2_mb.py env.exp_name=q1_cheetah_n5_arch2x256 env.env_name=cheetah-roble-v0 alg.num_agent_train_steps_per_iter=5 alg.network.layer_sizes='[256,256]' alg.n_iter=1 logging.video_log_freq=1
+```
+
+Network of 2x256 hidden layers, with 500 training steps
+```
+python run_hw2_mb.py env.exp_name=q1_cheetah_n500_arch2x256 env.env_name=cheetah-roble-v0 alg.num_agent_train_steps_per_iter=500 alg.network.layer_sizes='[256,256]' alg.n_iter=1 logging.video_log_freq=1
+```
+
+## Question 2
+
+```
+python run_hw2_mb.py env.exp_name=q2_obstacles_singleiteration env.env_name=obstacles-roble-v0 alg.num_agent_train_steps_per_iter=20 alg.batch_size_initial=5000 alg.batch_size=1000 alg.mpc_horizon=10 alg.n_iter=1 logging.video_log_freq=1
+```
+
+## Question 3
+
+
+MBRL run on obstacles env.
+```
+python run_hw2_mb.py env.exp_name=q3_obstacles env.env_name=obstacles-roble-v0 alg.num_agent_train_steps_per_iter=20 alg.batch_size_initial=5000 alg.batch_size=1000 alg.mpc_horizon=10 alg.n_iter=12 logging.video_log_freq=11
+```
+
+MBRL run on reacher env.
+```
+python run_hw2_mb.py env.exp_name=q3_reacher env.env_name=reacher-roble-v0 alg.mpc_horizon=10 alg.num_agent_train_steps_per_iter=1000 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=15 logging.video_log_freq=14
+```
+
+MBRL run on cheetah env.
+```
+python run_hw2_mb.py env.exp_name=q3_cheetah env.env_name=cheetah-roble-v0 alg.mpc_horizon=15  alg.num_agent_train_steps_per_iter=1500 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=20 logging.video_log_freq=19
+```
+
+## Question 4
+
+
+Horizon 5 steps.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_horizon5 env.env_name=reacher-roble-v0 alg.add_sl_noise=true alg.mpc_horizon=5 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 logging.video_log_freq=14
+```
+
+Horizon 15 steps.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_horizon15 env.env_name=reacher-roble-v0 alg.add_sl_noise=true alg.mpc_horizon=15 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 logging.video_log_freq=14
+```
+
+Horizon 30 steps.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_horizon30 env.env_name=reacher-roble-v0 alg.add_sl_noise=true alg.mpc_horizon=30 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 logging.video_log_freq=14
+```
+
+100 action sequences.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_numseq100 env.env_name=reacher-roble-v0 alg.add_sl_noise=true alg.mpc_horizon=10 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 alg.mpc_num_action_sequences=100 logging.video_log_freq=14
+```
+
+1000 action sequences.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_numseq1000 env.env_name=reacher-roble-v0 alg.add_sl_noise=true alg.mpc_horizon=10 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 alg.mpc_num_action_sequences=1000 logging.video_log_freq=14
+```
+
+No ensemble.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_ensemble1 env.env_name=reacher-roble-v0 alg.ensemble_size=1 alg.add_sl_noise=true alg.mpc_horizon=10 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 logging.video_log_freq=14
+```
+
+Ensemble size 3.
+```
+python run_hw2_mb.py env.exp_name=q4_reacher_ensemble3 env.env_name=reacher-roble-v0 alg.ensemble_size=3 alg.add_sl_noise=true alg.mpc_horizon=10 alg.mpc_action_sampling_strategy='random' alg.num_agent_train_steps_per_iter=1000 alg.batch_size=800 alg.n_iter=15 logging.video_log_freq=14
+```
+
+## Question 5
+
+MBRL with random shooting
+```
+python run_hw2_mb.py env.exp_name=q5_cheetah_random env.env_name='cheetah-roble-v0' alg.mpc_horizon=15  alg.num_agent_train_steps_per_iter=1500 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=5 logging.video_log_freq=4 alg.mpc_action_sampling_strategy='random'
+```
+
+MBRL with CEM 2
+```
+python run_hw2_mb.py env.exp_name=q5_cheetah_cem_2 env.env_name='cheetah-roble-v0' alg.mpc_horizon=15 alg.add_sl_noise=true alg.num_agent_train_steps_per_iter=1500 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=5 logging.video_log_freq=4 alg.mpc_action_sampling_strategy='cem' alg.cem_iterations=2
+```
+
+
+MBRL with CEM 4
+```
+python run_hw2_mb.py env.exp_name=q5_cheetah_cem_4 env.env_name='cheetah-roble-v0' alg.mpc_horizon=15 alg.add_sl_noise=true alg.num_agent_train_steps_per_iter=1500 alg.batch_size_initial=5000 alg.batch_size=5000 alg.n_iter=5 logging.video_log_freq=4 alg.mpc_action_sampling_strategy='cem' alg.cem_iterations=4
+```
\ No newline at end of file
diff --git a/hw2/plots_generator.py b/hw2/plots_generator.py
new file mode 100644
index 0000000..89ce796
--- /dev/null
+++ b/hw2/plots_generator.py
@@ -0,0 +1,119 @@
+import os
+import pandas as pd
+import matplotlib.pyplot as plt
+import numpy as np
+
+
+def generate_plots(base_dir='hw2/data'):
+    for root, dirs, files in os.walk(base_dir):
+        for file in files:
+            if file == 'log_data.csv':
+                csv_path = os.path.join(root, file)
+                log_data = pd.read_csv(csv_path)
+                
+                eval_returns = log_data['trainer/Eval_AverageReturn']
+                train_returns = log_data['trainer/Train_AverageReturn']
+                
+                x_values = np.arange(1,1+len(eval_returns))
+
+                plt.plot(x_values, eval_returns, 'o-', label='Evaluation Returns')
+                plt.plot(x_values, train_returns, 'o-', label='Training Returns')
+
+                plt.legend()
+                plt.title('Training and Evaluation Returns Over Time')
+                plt.xlabel('Epochs')
+                plt.ylabel('Average Return')
+                plt.xticks(x_values)
+
+                plt.show()
+
+def generate_q4_plot(base_dir='hw2/data'):
+
+    horizon5_path = os.path.join(base_dir, 'hw2_q4_reacher_horizon5/log_data.csv')
+    horizon15_path = os.path.join(base_dir, 'hw2_q4_reacher_horizon15/log_data.csv')
+    horizon30_path = os.path.join(base_dir, 'hw2_q4_reacher_horizon30/log_data.csv')
+
+    numseq100_path = os.path.join(base_dir, 'hw2_q4_reacher_numseq100/log_data.csv')
+    numseq1000_path = os.path.join(base_dir, 'hw2_q4_reacher_numseq1000/log_data.csv')
+
+    ensemble1_path = os.path.join(base_dir, 'hw2_q4_reacher_ensemble1/log_data.csv')
+    ensemble3_path = os.path.join(base_dir, 'hw2_q4_reacher_ensemble3/log_data.csv')
+
+    horizon5_returns = pd.read_csv(horizon5_path)['trainer/Eval_AverageReturn']
+    horizon15_returns = pd.read_csv(horizon15_path)['trainer/Eval_AverageReturn']
+    horizon30_returns = pd.read_csv(horizon30_path)['trainer/Eval_AverageReturn']
+
+    x_values = np.arange(1,1+len(horizon5_returns))
+
+    plt.plot(x_values, horizon5_returns, label='5 steps')
+    plt.plot(x_values, horizon15_returns, label='15 steps')
+    plt.plot(x_values, horizon30_returns, label='30 steps')
+
+    plt.legend()
+    plt.title('Return with differents horizon steps')
+    plt.xlabel('Epochs')
+    plt.ylabel('Average Return')
+    plt.xticks(x_values)
+
+    plt.show()
+    plt.clf()
+
+    numseq100_returns = pd.read_csv(numseq100_path)['trainer/Eval_AverageReturn']
+    numseq1000_returns = pd.read_csv(numseq1000_path)['trainer/Eval_AverageReturn']
+
+    plt.plot(x_values, numseq100_returns, label='100 sequences')
+    plt.plot(x_values, numseq1000_returns, label='1000 sequences')
+
+    plt.legend()
+    plt.title('Return with differents actions sequences')
+    plt.xlabel('Epochs')
+    plt.ylabel('Average Return')
+    plt.xticks(x_values)
+
+    plt.show()
+    plt.clf()
+
+    ensemble1_returns = pd.read_csv(ensemble1_path)['trainer/Eval_AverageReturn']
+    ensemble3_returns = pd.read_csv(ensemble3_path)['trainer/Eval_AverageReturn']
+
+    plt.plot(x_values, ensemble1_returns, label='No ensemble')
+    plt.plot(x_values, ensemble3_returns, label='Ensemble of 3')
+
+    plt.legend()
+    plt.title('Return with differents ensembles')
+    plt.xlabel('Epochs')
+    plt.ylabel('Average Return')
+    plt.xticks(x_values)
+
+    plt.show()
+
+
+
+def generate_q5_plot(base_dir='hw2/data'):
+
+    rs_file = os.path.join(base_dir, 'hw2_q5_cheetah_random/log_data.csv')
+    cem2_file = os.path.join(base_dir, 'hw2_q5_cheetah_cem_2/log_data.csv')
+    cem4_file = os.path.join(base_dir, 'hw2_q5_cheetah_cem_4/log_data.csv')
+
+    rs_returns = pd.read_csv(rs_file)['trainer/Eval_AverageReturn']
+    cem2_returns = pd.read_csv(cem2_file)['trainer/Eval_AverageReturn']
+    cem4_returns = pd.read_csv(cem4_file)['trainer/Eval_AverageReturn']
+
+    x_values = np.arange(1,1+len(rs_returns))
+
+    plt.plot(x_values, rs_returns, label='Random shooting')
+    plt.plot(x_values, cem2_returns, label='CEM 2')
+    plt.plot(x_values, cem4_returns, label='CEM 4')
+
+    plt.legend()
+    plt.title('Evaluation Returns Over Time')
+    plt.xlabel('Epochs')
+    plt.ylabel('Average Return')
+    plt.xticks(x_values)
+
+    plt.show()
+
+if __name__ == '__main__':
+    generate_plots()
+    generate_q4_plot()
+    generate_q5_plot()
diff --git a/hw2/roble/agents/mb_agent.py b/hw2/roble/agents/mb_agent.py
index 28144f8..7bd731e 100644
--- a/hw2/roble/agents/mb_agent.py
+++ b/hw2/roble/agents/mb_agent.py
@@ -14,7 +14,7 @@ import numpy as np
 class MBAgent(BaseAgent):
     import hw1.roble.util.class_util as classu
     @classu.hidden_member_initialize
-    def __init__(self, env, params):
+    def __init__(self, env, params, **kwargs):
         super(MBAgent, self).__init__()
 
         self._env = env.unwrapped
@@ -41,16 +41,21 @@ class MBAgent(BaseAgent):
         num_data = ob_no.shape[0]
         num_data_per_ens = int(num_data / self._params["ensemble_size"])
 
+        # perm = np.random.permutation(len(ob_no))
+
         for i in range(self._params["ensemble_size"]):
             # select which datapoints to use for this model of the ensemble
             # you might find the num_data_per_env variable defined above useful
 
-            # observations = # TODO(Q1)
-            # actions = # TODO(Q1)
-            # next_observations = # TODO(Q1)
+            start_idx = i * num_data_per_ens
+            end_idx = start_idx + num_data_per_ens
+
+            observations = ob_no[start_idx:end_idx]
+            actions = ac_na[start_idx:end_idx]
+            next_observations = next_ob_no[start_idx:end_idx]
 
             # # use datapoints to update one of the dyn_models
-            # model =  # TODO(Q1)
+            model =  self._dyn_models[i]
             log = model.update(observations, actions, next_observations,
                                 self._data_statistics)
             loss = log['Training Loss']
diff --git a/hw2/roble/envs/obstacles/obstacles_env.py b/hw2/roble/envs/obstacles/obstacles_env.py
index 19f7f83..5330670 100644
--- a/hw2/roble/envs/obstacles/obstacles_env.py
+++ b/hw2/roble/envs/obstacles/obstacles_env.py
@@ -5,6 +5,8 @@ from gym import spaces
 class Obstacles(gym.Env):
     def __init__(self, start=[-0.5, 0.75], end=[0.7, -0.8], random_starts=True):
 
+        import matplotlib
+        matplotlib.use('Agg')
         import matplotlib.pyplot as plt #inside, so doesnt get imported when not using this env
         self.plt = plt
 
@@ -72,7 +74,9 @@ class Obstacles(gym.Env):
 
         #clear
         self.counter = 0
-        self.plt.clf()
+        # self.plt.clf()
+        self.plt.close(self.fig)
+        self.fig = self.plt.figure()
 
         #return
         return self._get_obs()
@@ -189,6 +193,7 @@ class Obstacles(gym.Env):
         self.fig.canvas.draw()
         img = np.fromstring(self.fig.canvas.tostring_rgb(), dtype=np.uint8)
         img = img.reshape(self.fig.canvas.get_width_height()[::-1] + (3,))
+        # img = img.reshape(960,1280,3)
         return img
 
     def is_valid(self, dat):
diff --git a/hw2/roble/infrastructure/rl_trainer.py b/hw2/roble/infrastructure/rl_trainer.py
index 9187004..8c05ef3 100644
--- a/hw2/roble/infrastructure/rl_trainer.py
+++ b/hw2/roble/infrastructure/rl_trainer.py
@@ -129,6 +129,7 @@ class RL_Trainer(RL_Trainer):
                 if self._params['logging']['save_params']:
                     self._agent.save('{}/agent_itr_{}.pt'.format(self._params['logging']['logdir'], itr))
         
+        self.plot_return()
         return self._logger.get_table_dict()
 
     def collect_training_trajectories(
@@ -279,7 +280,7 @@ class RL_Trainer(RL_Trainer):
         # plot the predictions
         self._fig.clf()
         for i in range(ob_dim):
-            plt.subplot(ob_dim/2, 2, i+1)
+            plt.subplot(int(ob_dim/2), 2, i+1)
             plt.plot(true_states[:,i], 'g')
             plt.plot(pred_states[:,i], 'r')
         self._fig.suptitle('MPE: ' + str(mpe))
@@ -292,3 +293,23 @@ class RL_Trainer(RL_Trainer):
         plt.plot(all_losses)
         self._fig.savefig(self._params['logging']['logdir']+'/itr_'+str(itr)+'_losses.png', dpi=200, bbox_inches='tight')
 
+
+    def plot_return(self):
+        import matplotlib.pyplot as plt
+        self._fig = plt.figure()
+
+        eval_returns = self._logger.get_table_dict()['trainer/Eval_AverageReturn']
+        train_returns = self._logger.get_table_dict()['trainer/Train_AverageReturn']
+        x_values = np.arange(1,1+len(eval_returns))
+
+        plt.plot(x_values, eval_returns, 'go-', label='Evaluation Returns')
+        plt.plot(x_values, train_returns, 'bo-', label='Training Returns')
+
+        plt.legend()
+
+        plt.title('Training and Evaluation Returns Over Time')
+        plt.xlabel('Epoch')
+        plt.ylabel('Average Return')
+        plt.xticks(x_values)
+
+        self._fig.savefig(self._params['logging']['logdir']+'/returns.png', dpi=200, bbox_inches='tight')
\ No newline at end of file
diff --git a/hw2/roble/models/ff_model.py b/hw2/roble/models/ff_model.py
index 1c08b28..8965444 100644
--- a/hw2/roble/models/ff_model.py
+++ b/hw2/roble/models/ff_model.py
@@ -15,7 +15,7 @@ class FFModel(nn.Module, BaseModel):
         self._delta_network = ptu.build_mlp(
             input_size=self._ob_dim + self._ac_dim,
             output_size=self._ob_dim,
-            **params["network"]
+            params=self._network
         )
         self._delta_network.to(ptu.device)
         self._optimizer = optim.Adam(
@@ -73,8 +73,15 @@ class FFModel(nn.Module, BaseModel):
                 unnormalized) output of the delta network. This is needed
         """
         # normalize input data to mean 0, std 1
-        # obs_normalized = # TODO(Q1)
-        # acs_normalized = # TODO(Q1)
+
+        obs_unnormalized = ptu.from_numpy(obs_unnormalized)
+        acs_unnormalized = ptu.from_numpy(acs_unnormalized)
+
+        obs_normalized = normalize(obs_unnormalized, self._obs_mean, self._obs_std)
+        acs_normalized = normalize(acs_unnormalized, self._acs_mean, self._acs_std)
+
+        # obs_normalized = (obs_unnormalized - self._obs_mean) / self._obs_std
+        # acs_normalized = (acs_unnormalized - self._acs_mean) / self._acs_std
 
         # predicted change in obs
         concatenated_input = torch.cat([obs_normalized, acs_normalized], dim=1)
@@ -82,9 +89,27 @@ class FFModel(nn.Module, BaseModel):
         # TODO(Q1) compute delta_pred_normalized and next_obs_pred
         # Hint: as described in the PDF, the output of the network is the
         # *normalized change* in state, i.e. normalized(s_t+1 - s_t).
-        # delta_pred_normalized = # TODO(Q1)
-        # next_obs_pred = # TODO(Q1)
+
+        delta_pred_normalized = self._delta_network(concatenated_input)
+        delta_pred = unnormalize(delta_pred_normalized, self._delta_mean, self._delta_std)
+        # delta_pred = delta_pred_normalized * self._delta_std + self._delta_mean
+
+        next_obs_pred = obs_unnormalized + delta_pred
         return next_obs_pred, delta_pred_normalized
+        
+        # obs_normalized = ptu.from_numpy((obs_unnormalized - obs_mean) / obs_std)
+        # acs_normalized = ptu.from_numpy((acs_unnormalized - acs_mean) / acs_std)
+
+        # # predicted change in obs
+        # concatenated_input = torch.cat([obs_normalized, acs_normalized], dim=1)
+
+        # # TODO(Q1) compute delta_pred_normalized and next_obs_pred
+        # # Hint: as described in the PDF, the output of the network is the
+        # # *normalized change* in state, i.e. normalized(s_t+1 - s_t).
+        # delta_pred_normalized = self._delta_network(concatenated_input)
+        # delta_pred = delta_pred_normalized * ptu.from_numpy(delta_std) + ptu.from_numpy(delta_mean)
+        # next_obs_pred = obs_unnormalized + delta_pred
+        # return next_obs_pred, delta_pred_normalized
 
     def get_prediction(self, obs, acs, data_statistics):
         """
@@ -103,7 +128,13 @@ class FFModel(nn.Module, BaseModel):
         # prediction = # TODO(Q1) get numpy array of the predicted next-states (s_t+1)
         # Hint: `self(...)` returns a tuple, but you only need to use one of the
         # outputs.
-        return prediction
+
+        # self.update_statistics(**data_statistics)
+        # next_obs_pred, _ = self(ptu.from_numpy(obs), ptu.from_numpy(acs), **data_statistics)
+
+        next_obs_pred, _ = self(obs, acs, **data_statistics)
+
+        return ptu.to_numpy(next_obs_pred)
 
     def update(self, observations, actions, next_observations, data_statistics):
         """
@@ -128,6 +159,22 @@ class FFModel(nn.Module, BaseModel):
         # loss = # TODO(Q1) compute the loss
         # Hint: `self(...)` returns a tuple, but you only need to use one of the
         # outputs.
+
+        self.update_statistics(**data_statistics)
+
+        true_delta = ptu.from_numpy(next_observations - observations)
+        # target_norm = (true_delta - self._delta_mean) / self._delta_std
+        target_norm = normalize(true_delta, self._delta_mean, self._delta_std)
+
+        # target_norm = (true_delta - true_delta.mean()) / true_delta.std()
+        # target_norm = ptu.from_numpy(target_norm)
+
+        
+    
+        _, delta_pred_norm = self(observations, actions, **data_statistics)
+
+        loss = self._loss(delta_pred_norm, target_norm)
+
         self._optimizer.zero_grad()
         loss.backward()
         self._optimizer.step()
diff --git a/hw2/roble/policies/MPC_policy.py b/hw2/roble/policies/MPC_policy.py
index 619b622..a5ef657 100644
--- a/hw2/roble/policies/MPC_policy.py
+++ b/hw2/roble/policies/MPC_policy.py
@@ -25,6 +25,8 @@ class MPCPolicy(BasePolicy):
         self._low = self._ac_space.low
         self._high = self._ac_space.high
 
+        # self.models = dyn_models
+
         # Sampling strategy
         allowed_sampling = ('random', 'cem')
         assert self._mpc_action_sampling_strategy in allowed_sampling, f"self._mpc_action_sampling_strategy must be one of the following: {allowed_sampling}"
@@ -44,13 +46,15 @@ class MPCPolicy(BasePolicy):
             # TODO (Q1) uniformly sample trajectories and return an array of
             # dimensions (num_sequences, horizon, self._ac_dim) in the range
             # [self._low, self._high]
-            TODO
+            random_action_sequences = np.random.uniform(low=self._low, high=self._high,
+                                                        size=(num_sequences, horizon, self._ac_space.shape[0]))
             return random_action_sequences
         elif self._mpc_action_sampling_strategy == 'cem':
             # TODO(Q5): Implement action selection using CEM.
             # Begin with randomly selected actions, then refine the sampling distribution
             # iteratively as described in Section 3.3, "Iterative Random-Shooting with Refinement" of
             # https://arxiv.org/pdf/1909.11652.pdf
+
             for i in range(self._cem_iterations):
                 # - Sample candidate sequences from a Gaussian with the current
                 #   elite mean and variance
@@ -60,25 +64,44 @@ class MPCPolicy(BasePolicy):
                 #     (Hint: what existing function can we use to compute rewards for
                 #      our candidate sequences in order to rank them?)
                 # - Update the elite mean and variance
-                pass
+                
+                if i == 0:
+                    candidates = np.random.uniform(low=self._low, high=self._high,
+                                                        size=(num_sequences, horizon, self._ac_space.shape[0]))
+                else:
+                    candidates = np.random.normal(elites_mean, elites_std,(num_sequences, horizon, self._ac_space.shape[0]))
+            
+                scores = self.evaluate_candidate_sequences(candidates, obs)
+
+                elites_ids = scores.argsort()[-self._cem_num_elites:]
+
+                elites = candidates[elites_ids]
+
+                elites_mean = elites.mean(axis=0)
+                elites_std = elites.std(axis=0)
+
+                
 
             # TODO(Q5): Set `cem_action` to the appropriate action sequence chosen by CEM.
             # The shape should be (horizon, self._ac_dim)  
-            cem_action = None
+            cem_action = elites_mean
             return cem_action[None]
         else:
             raise Exception(f"Invalid sample_strategy: {self._mpc_action_sampling_strategy}")
-
+        
     def evaluate_candidate_sequences(self, candidate_action_sequences, obs):
         # TODO(Q2): for each model in ensemble, compute the predicted sum of rewards
         # for each candidate action sequence.
         #
         # Then, return the mean predictions across all ensembles.
         # Hint: the return value should be an array of shape (N,)
+        N, _, _ = candidate_action_sequences.shape
+        predicted_rewards = []
+
         for model in self._dyn_models:
-            pass
+            predicted_rewards.append(self.calculate_sum_of_rewards(obs, candidate_action_sequences, model))
 
-        return TODO
+        return np.array(predicted_rewards).mean(axis=0)
 
     def get_action(self, obs):
         if self._data_statistics is None:
@@ -94,8 +117,9 @@ class MPCPolicy(BasePolicy):
         else:
             predicted_rewards = self.evaluate_candidate_sequences(candidate_action_sequences, obs)
             # pick the action sequence and return the 1st element of that sequence
-            best_action_sequence = None  # TODO (Q2)
-            action_to_take = None  # TODO (Q2)
+            best_sequence_idx = np.argmax(predicted_rewards)
+            best_action_sequence = candidate_action_sequences[best_sequence_idx]
+            action_to_take = best_action_sequence[0]
             return action_to_take[None]  # Unsqueeze the first index
 
     def calculate_sum_of_rewards(self, obs, candidate_action_sequences, model):
@@ -123,4 +147,63 @@ class MPCPolicy(BasePolicy):
         # Hint: Remember that the model can process observations and actions
         #       in batch, which can be much faster than looping through each
         #       action sequence.
+
+        # obs_sequences = []
+        # obs_sequences.append(np.repeat(obs, candidate_action_sequences.shape[0]))
+
+        # cas = np.transpose(candidate_action_sequences, (1, 0, 2))
+        # for actions in cas:
+        #     predicted_obs = model(actions)
+        #     self._env.get_reward(predicted_obs, action)
+
+        # model
+        
+        # return sum_of_rewards
+
+
+
+        # N, H, _ = candidate_action_sequences.shape  # N: Number of sequences, H: Horizon
+        # sum_of_rewards = np.zeros(N)  # Initialize the sum of rewards for each sequence
+        
+        # # Iterate over each action sequence
+        # for i in range(N):
+        #     cumulative_reward = 0  # Initialize cumulative reward for the sequence
+        #     current_obs = obs.copy()  # Start with the initial observation
+            
+        #     # Iterate over each time step in the horizon
+        #     for h in range(H):
+        #         # Extract the current action from the sequence
+        #         action = candidate_action_sequences[i, h, :]
+                
+        #         # Predict the next state based on the current state and action
+        #         next_obs_pred = model.get_prediction(current_obs[None, :], action[None, :], self._data_statistics)
+                
+        #         # Calculate the reward for the predicted state and current action
+        #         reward, _ = self._env.get_reward(current_obs[None, :], action[None, :])
+                
+        #         cumulative_reward += reward  # Accumulate the reward
+                
+        #         # Update the current observation to the predicted next observation
+        #         current_obs = next_obs_pred.squeeze()  # Assuming next_obs_pred is of shape (1, D_obs)
+            
+        #     # Store the sum of rewards for this action sequence
+        #     sum_of_rewards[i] = cumulative_reward
+        
+        # return sum_of_rewards
+
+
+        N, H, _ = candidate_action_sequences.shape
+        candidate_action_sequences = candidate_action_sequences.transpose(1, 0, 2)
+        sum_of_rewards = np.zeros(N)
+        current_obs = np.tile(obs, (N, 1))
+
+        for h in range(H):
+            actions = candidate_action_sequences[h]
+            next_obs_pred = model.get_prediction(current_obs, actions, self._data_statistics)
+
+            rewards, _ = self._env.get_reward(current_obs, actions)
+            sum_of_rewards += rewards.squeeze()
+
+            current_obs = next_obs_pred
+
         return sum_of_rewards
